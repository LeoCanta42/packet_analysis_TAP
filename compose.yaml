services:
  logstash:
    hostname: logstash
    image: "docker.elastic.co/logstash/logstash:8.13.2"
    volumes:
      - ./logstash.conf/:/usr/share/logstash/pipeline/logstash.conf
      - ./packets.log:/usr/share/logstash/packets.log
    environment:
      XPACK_MONITORING_ENABLED: "false"
  ##
  zookeeper:
    hostname: zookeeper
    image: "confluentinc/cp-zookeeper:latest"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
  ##
  kafkaserver:
    hostname: kafkaserver
    image: "confluentinc/cp-kafka:latest"
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafkaserver:9092
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  ##
  init-kafka:
    image: "confluentinc/cp-kafka:latest"
    depends_on:
      - kafkaserver
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafkaserver:9092 --create --if-not-exists --topic packets --replication-factor 1 --partitions 1
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafkaserver:9092 --list
      "
  ##
  # kafkaui:
  #   hostname: kafkaui
  #   image: "provectuslabs/kafka-ui:latest"
  #   ports:
  #     - 8080:8080
  #   depends_on:
  #     - kafkaserver
  #   environment:
  #     KAFKA_CLUSTERS_0_NAME: "local"
  #     KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "PLAINTEXT://kafkaserver:9092"
  ##
  spark:
    hostname: spark
    build: ./spark
    depends_on:
      init-kafka:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_started
    volumes:
      - ./spark/packet_analyzer_FULL.py:/opt/spark/work-dir/packet_analyzer.py
      - ./spark/abuseipdb.key:/opt/spark/work-dir/abuseipdb.key
      - ./spark/ipinfotoken.key:/opt/spark/work-dir/ipinfotoken.key
      - ./anomaly_detection/kmeans_model.pkl:/opt/spark/work-dir/kmeans_model.pkl
      - ./anomaly_detection/scaler_model.pkl:/opt/spark/work-dir/scaler_model.pkl
      - ./anomaly_detection/feature_names.pkl:/opt/spark/work-dir/feature_names.pkl
    # command: "/opt/spark/bin/spark-submit --conf spark.driver.extraJavaOptions='-Divy.cache.dir=/tmp -Divy.home=/tmp' --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4 /opt/spark/work-dir/packet_analyzer.py"
    command: "/opt/spark/run-spark.sh"
    ##
  elasticsearch:
    hostname: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    mem_limit: 4 GB
    ports:
      - 9200:9200
    volumes:
      - esdata:/usr/share/elasticsearch/data
  ##
  kibana:
    hostname: kibana
    image: docker.elastic.co/kibana/kibana:8.13.4
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - 5601:5601
  ##

volumes:
  esdata:
